<!-- <!DOCTYPE html>
<html>
  <body>

    <h1> Test Heading </h1>
    <p> Test Paragraph </p>

  </body>
</html>

 -->


<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="../../favicon.ico">

    <title>CS 152: Final Project</title>

    <!-- Bootstrap core CSS -->
    <link href="bootstrap-3.3.7-dist/css/bootstrap.min.css" rel="stylesheet">
    
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="bootstrap-3.3.7-dist/ie/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <link href="adjustJumbotron.css?v=4" rel="stylesheet">

  </head>

  <body>

    <!-- Fixed navbar -->
    <nav class="navbar navbar-default navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand">Spatial Transformer Networks</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li><a href="problem.html">Problem Statement</a></li>
            <li class="active"><a href="background.html">Background</a></li>
            <li><a href="experiment.html">Experimental Design</a></li>
            <li><a href="results.html">Results</a></li>
            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">More<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="references.html">References</a></li>
                <li class><a href="code.html">Code Directory</a></li>
                <li><a href="present.html">Presentations</a></li>
              </ul>
            </li>
          </ul>
        </div>
      </div>
    </nav>


    <div class="container">

      <div class="jumbotron">
        <h1>Background</h1>
        <p>
        The primary text of interest our project is focused on is <a href="STN_paper.pdf">Spatial Transformer Networks</a> by Jaderburg et al. 
        This paper primarily presents the idea of Spatial Transformers and using them to improve the performance of convolutional neural network (CNN)-based digit identifiers. 
        In this section, we provide background on the spatial transformers and how they work.
        </p>

        <h2> What are Spatial Transformers</h2>
        <p>
        Spatial Transformers are modules that can be inserted anywhere within a convolutional neural network setup that specifically learns how to reorient characters and images for better digit recognition performance. They consist of three main parts: a localization network, a grid generator, and sampler. Let us describe each part of the Spatial Transformer.
        </p>

        <center><img src="images/spatial_trans.png" alt="spatial transformer diagram" style="width:75%;"></center>

        <h3>Localization Network</h3>
        <p>
        The localization network is essentially a function that takes in the input image and outputs the parameters of the transformation to be applied to the feature map. The paper suggests using a fully-connected network or a convolutional network to learn the parameters with each image; however the exact implementation is left up to the implementer and could be an area of additional investigation.
        <br><br>
        The diagram below summarizes the input and output of the localization network.
        </p>

        <center><img src="images/local_network.png" alt="localization network" style="width:75%;"></center>

        <h3>Grid Generator</h3>
        <p>
        The grid generator creates a grid that maps points from the input image to points on the re-oriented output image. Using the parameters output from the Localization Network, we compute a modified grid that only takes certain points from the original image.
        <br><br>
        In the diagram below, we see an example of how this grid works. In figure (a), the identity parameters are passed into the transform function, resulting in the same image output. In figure (b), the learned parameters are passed into the transform function. The resulting image (V) is a set of points from the original image, ideally upscaled and reoriented in such a way that the image is now centered and upright.
        </p>

        <center><img src="images/grid1.png" alt="Example of Grid Generation" style="width:75%;"></center>

        <p>
        The mathematics of the grid generator can be summarized in the following equation.  
        </p>
        <center><img src="images/grid2.png" alt="Equation of Grid Generation" style="width:75%;"></center>
        <p>
        Using the matrix of theta values, which is obtained through the localization network, we can determine how each pixel in the new image maps from the original image. <br>
        <!--Note that the equation above is backwards as <i>(x<sub>i</sub><sup>s</sup>,y<sub>i</sub><sup>s</sup>)</i> corresponds to the <i>i</i>-th point on the starting image (U) while <i>(x<sub>i</sub><sup>t</sup>,y<sub>i</sub><sup>t</sup>)</i> corresponds to the <i>i</i>-th point on the output target image (V). -->
        </p>

        <h3>Sampler</h3> 
        <p>
        The sampler uses the input image and the grid of sampling points from the grid generator to sample from the original image and create an output image. This process is described by the following equation:
        </p>
        <center><img src="images/sampler.png" alt="Equation of Sampler" style="width:75%;"></center>
        <p>
<!--         Note that <i>(x<sub>i</sub><sup>s</sup>,y<sub>i</sub><sup>s</sup>)</i> corresponds to the point in the original image that the grid generator maps to the output point in question. <br><br> -->
        This equation essentially states that for the i-th point in the output image, we scan through the initial image, applying a sampling kernel, <i>k</i> and takes the sum of the result of applying this kernel to every pixel in the input image as its output. <br><br>

        The choice of sampling kernel is left up to the choice of the implementer; however the paper gives us the example of using the integer sampling kernel, which yields the equation shown below. They note that this function essentially equates to copying the value of the  <i>(x<sub>i</sub><sup>s</sup>,y<sub>i</sub><sup>s</sup>)</i> to <i>(x<sub>i</sub><sup>t</sup>,y<sub>i</sub><sup>t</sup>)</i>.
        </p>
        <center><img src="images/sampler2.png" alt="Equation of Sampler with Kernel" style="width:75%;"></center>

      <h2>Integrating Spatial Transformers with CNNs</h2>
        <p>
        Spatial transformers typically output feature maps with the same dimensions as the input feature map. This would allow for easy integration into pre-existing networks, since a spatial transformer can be seamlessly inserted before a CNN, with no changes to the CNN code. As described in <a href="STN_paper.pdf">Spatial Transformer Networks</a> by Jaderburg et al., the spatial transformer can help actively transform the feature map to minimize a given cost function. The spatial transformer can also downsample or upsample a feature map, although this would change the dimensions of the output image resulting in more overhead in integration of pre-existing networks. The diagram below shows a summary of adding an STN to the CNN pipeline.
        </p>

        <center><img src="images/integrate.png" alt="stn + cnn diagram" style="width:75%;"></center>

        <p>
        Jaderburg et al. implement spatial transformers in series with fully connected networks (FCNs) and CNNs for digit identification. They report a reduced error of 13.2% to 2.0% for the FCN, and from 3.5% to 1.7% for the CNN.
        </p>

      <h2>Extension: Adding Spatial Transformers between CNN Layers</h2>
        <p>
        <a href="STN_paper.pdf">Spatial Transformer Networks</a> presents results of a spatial transformer being placed before a CNN for digit identification. However, Jaderburg et al. posit that this is not the only location a spatial transformer can be placed in the neural network chain. The paper suggests that spatial transformers can be placed between convolutional layers in a network, which will create abstract representations of the input feature map. This idea is explored in this project, discussed in detail in <a href="experiment.html">Experimental Design</a>. The diagram below shows how one may add an STN between layers of a CNN.
        </p>
        <center><img src="images/mstn.png" alt="multi-stn diagram" style="width:75%;"></center>

      </div>

    </div> <!-- /container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="bootstrap-3.3.7-dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="bootstrap-3.3.7-dist/ie/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
